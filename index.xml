<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DataFibers</title>
    <link>https://datafibers-community.github.io/</link>
    <description>Recent content on DataFibers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright (c) 2018, DataFibers all rights reserved.</copyright>
    <lastBuildDate>Fri, 02 Mar 2018 13:50:46 +0200</lastBuildDate>
    
	<atom:link href="https://datafibers-community.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ML Overview</title>
      <link>https://datafibers-community.github.io/blog/2018/03/02/2018-03-02-ml-overview/</link>
      <pubDate>Fri, 02 Mar 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/03/02/2018-03-02-ml-overview/</guid>
      <description>Background Machine learning is a field of computer science that gives computer systems the ability to &amp;ldquo;learn&amp;rdquo; with data, without being explicitly programmed. Machine learning can be broken down into three broad categories: Recommender, Classification, Clustering.
 Recommender—Recommender systems suggest items based on past behavior or interest. These items can be other users in a social network, or products and services in retail websites. There are some algorithm like Pearson correlation and euclidean distance.</description>
    </item>
    
    <item>
      <title>Spark RowID Generation</title>
      <link>https://datafibers-community.github.io/blog/2018/03/01/2018-03-01-spark-rowid-generation/</link>
      <pubDate>Thu, 01 Mar 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/03/01/2018-03-01-spark-rowid-generation/</guid>
      <description>Most of time, we need to generate a unique identifier column for dataframe. There are couple of ways doing that as follows.
https://stackoverflow.com/questions/35705038/how-do-i-add-an-persistent-column-of-row-ids-to-spark-dataframe
Solution 1 The most frequent way of doing it is to to firstly find the MAX of age in each SEX group and do SELF JOIN by matching SEX and the MAX age as follows. This will create two stages of jobs and NOT efficient.
&amp;gt; SELECT employee.</description>
    </item>
    
    <item>
      <title>Hive Get the Max/Min</title>
      <link>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</link>
      <pubDate>Fri, 02 Feb 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</guid>
      <description>Most of time, we need to find the max or min value of particular columns as well as other columns. For example, we have following employee table.
&amp;gt; SELECT name,sex_age.sex AS sex,sex_age.age AS age FROM employee; +----------+---------+------+ | name | sex | age | +----------+---------+------+ | Michael | Male | 30 | | Will | Male | 35 | | Shelley | Female | 27 | | Lucy | Female | 57 | | Steven | Male | 30 | +----------+---------+------+ 5 rows selected (75.</description>
    </item>
    
    <item>
      <title>Big Data Books Reviews</title>
      <link>https://datafibers-community.github.io/blog/2018/01/10/2018-01-10-reviews-big-data/</link>
      <pubDate>Wed, 10 Jan 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/01/10/2018-01-10-reviews-big-data/</guid>
      <description>Learning Spark SQL  Level Ent.
 Level Mid.
 Level Adv.  Published in Sep. 2017. Start reading it.

Learning Apache Flink  Level Ent. Level Mid.  There are very few books about Apache Flink. Besides offical document, this is a good one for people who wants to know Flink quicker. This book, published in the earlier of 2017, covers most of core topics for Flink with examples.</description>
    </item>
    
    <item>
      <title>2017 Winter Release</title>
      <link>https://datafibers-community.github.io/blog/2017/12/22/2017-12-22-df-winter-release/</link>
      <pubDate>Fri, 22 Dec 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/12/22/2017-12-22-df-winter-release/</guid>
      <description>Summary Before christmas, DataFibers has completed the winter release, which has more than 40+ changes requests applied. In this release, DataFibers is featured with first demo combined both data landing and transforming in real time with new web interface. In addition, the preview version of batch processing (by spark) is ready.
Details Below is the list of key changes in this release.
 New Web admin UI released using ReactJs based AOR.</description>
    </item>
    
    <item>
      <title>Hive RowID Generation</title>
      <link>https://datafibers-community.github.io/blog/2017/11/02/2017-11-02-hive-rowid-generation/</link>
      <pubDate>Thu, 02 Nov 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/11/02/2017-11-02-hive-rowid-generation/</guid>
      <description>Introduction It is quite often that we need a unique identifier for each single rows in the Apache Hive tables. This is quite useful when you need such columns as surrogate keys in data warehouse, as the primary key for data or use as system nature keys. There are following ways of doing that in Hive.
ROW_NUMBER() Hive have a couple of internal functions to achieve this. ROW_NUMBER function, which can generate row number for each partition of data.</description>
    </item>
    
    <item>
      <title>GIT Tips</title>
      <link>https://datafibers-community.github.io/blog/2017/11/01/2017-11-01-git-tips/</link>
      <pubDate>Wed, 01 Nov 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/11/01/2017-11-01-git-tips/</guid>
      <description>1. Git Cheat Sheet 2. Check in Git Modified But Untracked Content Recently, I migrate this site to Hexo. I download the theme from github to the Hexo project folder. I also keep the source code in the github in case I lost the source code. However, when I run the git add . and git status. It shows error messages saying the theme folder is not tracked content. Most time, I did not check the git status - bad habit.</description>
    </item>
    
    <item>
      <title>Apache Kafka Overview</title>
      <link>https://datafibers-community.github.io/blog/2017/10/05/2017-10-05-apache-kafka-overview/</link>
      <pubDate>Thu, 05 Oct 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/10/05/2017-10-05-apache-kafka-overview/</guid>
      <description>The big data processing started by focusing on the batch processing. Distributed data storage and querying tools like MapReduce, Hive, and Pig were all designed to process data in batches rather than continuously. Recently enterprises have discovered the power of analyzing and processing data and events as they happen instead of batches. Most traditional messaging systems, such as RabbitMq, neither scale up to handle big data in realtime nor use friendly with big data ecosystem.</description>
    </item>
    
    <item>
      <title>Scala Apply Method</title>
      <link>https://datafibers-community.github.io/blog/2017/09/15/2017-09-15-scala-apply-method/</link>
      <pubDate>Fri, 15 Sep 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/09/15/2017-09-15-scala-apply-method/</guid>
      <description>The apply methods in scala has a nice syntactic sugar. It allows us to define semantics like java array access for an arbitrary class.
For example, we create a class of RiceCooker and its method cook to cook rice. Whenever we need to cook rice, we could call this method.
class RiceCooker { def cook(cup_of_rice: Rice) = { cup_of_rice.isDone = true cup_of_rice } } val my_rice_cooker: RiceCooker = new RiceCooker() my_rice_cooker.</description>
    </item>
    
    <item>
      <title>2017 Summer Release</title>
      <link>https://datafibers-community.github.io/blog/2017/08/31/2017-08-31-df-summer-release/</link>
      <pubDate>Thu, 31 Aug 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/08/31/2017-08-31-df-summer-release/</guid>
      <description>Summary A little bit data, but DataFibers has completed the summer release of 2017 about right time. In this release, we have applied 30+ changes requests. In this release, DataFibers is featured with a preview of new web interface. In addition, couple of connectors are added/updated to preparing the demo later.
Details Below is the list of key changes in this release.
 Support Flink Table API and SQL API support Flink upgrade to v1.</description>
    </item>
    
    <item>
      <title>Simplify Big Data Streaming</title>
      <link>https://datafibers-community.github.io/blog/2017/07/20/2017-07-20-df-meetup-summer/</link>
      <pubDate>Thu, 20 Jul 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/07/20/2017-07-20-df-meetup-summer/</guid>
      <description>Here is our free training offered during 2017 summer meetup in Toronto, Canada.</description>
    </item>
    
    <item>
      <title>Spark Word Count Tutorial</title>
      <link>https://datafibers-community.github.io/blog/2017/07/01/2017-07-01-spark-word-count/</link>
      <pubDate>Sat, 01 Jul 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/07/01/2017-07-01-spark-word-count/</guid>
      <description>It is quite often to setup Apache Spark development environment through IDE. Since I do not cover much setup IDE details in my Spark course, I am here to give detail steps for developing the well known Spark word count example using scala API in Eclipse.
Environment  Apache Spark v1.6 Scala 2.10.4 Eclipse Scala IDE  Download Software Needed  Download the proper scala version and install it Download the Eclipse scala IDE from above link  Create A Scala Project  Open Scala Eclipse IDE.</description>
    </item>
    
    <item>
      <title>One Platform Initatives for Spark</title>
      <link>https://datafibers-community.github.io/blog/2017/06/24/2017-06-24-cloudera-launches-one-platform-initatives-to-advance-spark---copy/</link>
      <pubDate>Sat, 24 Jun 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/06/24/2017-06-24-cloudera-launches-one-platform-initatives-to-advance-spark---copy/</guid>
      <description>In the early of this September, the Chief Strategy Offer of Cloudera Mike Olson has announced that the next important initiatives for Couldera - One Platform to advance their investment on Apache Spark.
The Spark is originally invented by few guys who started up the Databrick. Later, Spark catches most attention from big data communities and companies by its high-performance in-memory computing framework, which can run on top of Hadoop Yarn.</description>
    </item>
    
    <item>
      <title>Constructor - Scala vs. Java</title>
      <link>https://datafibers-community.github.io/blog/2017/05/01/2017-05-01-scala-and-java-constructors/</link>
      <pubDate>Mon, 01 May 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/05/01/2017-05-01-scala-and-java-constructors/</guid>
      <description>1. Constructor With Parameters Java Code
public class Foo() { public Bar bar; public Foo(Bar bar) { this.bar = bar; } }  Scala Code
class Foo(val bar:Bar)  2. Constructor With Private Attribute Java Code
public class Foo() { private final Bar bar; public Foo(Bar bar) { this.bar = bar; } }  Scala Code
class Foo(private val bar:Bar)  3. Call Super Constructor Java Code
public class Foo() extends SuperFoo { public Foo(Bar bar) { super(bar); } }  Scala Code</description>
    </item>
    
    <item>
      <title>2016 Winter Release</title>
      <link>https://datafibers-community.github.io/blog/2016/12/29/2016-12-29-df-winter-release/</link>
      <pubDate>Thu, 29 Dec 2016 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2016/12/29/2016-12-29-df-winter-release/</guid>
      <description>Summary Before new year, DataFibers has completed the winter release of 2016, which has more than 20+ changes requests applied. In this release, DataFibers is featured with new api document and landing pages. In addition, the preview version of stream processing (by flink) is ready.
Details Below is the list of key changes in this release.
 Integrated REST API Document to the DF Application
 Added landing welcome page</description>
    </item>
    
    <item>
      <title>When to Disable Speculative Execution</title>
      <link>https://datafibers-community.github.io/blog/2016/11/22/2016-11-22-when-to-disable-speculative-execution/</link>
      <pubDate>Tue, 22 Nov 2016 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2016/11/22/2016-11-22-when-to-disable-speculative-execution/</guid>
      <description>Backgrounds This is the link from WikiMedia about what’s Speculative Execution. In Hadoop, the following parameters string are for this settings. And, they are true by default.
mapred.map.tasks.speculative.execution mapred.reduce.tasks.speculative.execution  When to Disable Most time, it helps. However, I am here to collect some scenario when we do not need it.
 Of course, when ever your cluster really in shortage of resource or for the purpose of experiment, we can disable them by setting them to false since “SE” really a big resource consumer It is generally advisable to turn off ”SE” for mapred jobs that use HBase as a source.</description>
    </item>
    
    <item>
      <title>Community</title>
      <link>https://datafibers-community.github.io/community/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://datafibers-community.github.io/community/</guid>
      <description>How to Join Us  Get to know about the project at datafibers.org | datafibers.com Watch and Star us in GitHub Fork and Pull Request when you have ideas to contribute Contact Us for participant at datafibers@gmail.com Join our discussion and ask questions at datafibers@googlegroups.com Hear our news and events @ Facebook or Twitter Our Meetup is incoming  Our Git Repositories  datafibers data service - Core DataFibers service df_demo - This is where we build demo for DataFibers and other tookit df_complete_guide - This is the source for DataFibers complete guide  Steps to Contribute Your Code  Fork DF master branch to your own repository Work on the code with your preferred Java IDE Once done, check in your code as a monthly development individual branch called: development_YourName_FeatureName_YYYYMM to your own repository Create a full request to the monthly development branch called: development_YYYYMM from your own branch Do proper code review, merge and comments until you can successfully merge your code At every month end, our release lead will merge the monthly development branch to the master   In case you haven&amp;rsquo;t found the answer for your question please feel free to contact us or check out DataFibers Complete Guideline.</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>https://datafibers-community.github.io/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://datafibers-community.github.io/contact/</guid>
      <description>We are here to help you Would you like to join our community? Do you have some kind of problem with DataFibers? Are you interested in attending our meetups and trainings?
Please feel free to contact us. Your suggestions are always our treasure. We&amp;rsquo;ll try best to contact you as soon as possible.</description>
    </item>
    
    <item>
      <title>Training</title>
      <link>https://datafibers-community.github.io/training/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://datafibers-community.github.io/training/</guid>
      <description>Why to Take Our Trainings With the help of community efforts, DataFibers is able to offer the best quality of big data training with low cost. Currently, we have trained many professionals in Canada and China. We are also closely partnerd with top of leading training provides to polish and systematize our training practice. We are proud of delivering following values to our students as welll as customers as follows.</description>
    </item>
    
  </channel>
</rss>