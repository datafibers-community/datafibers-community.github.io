<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spark on DataFibers</title>
    <link>https://datafibers-community.github.io/tags/spark/</link>
    <description>Recent content in Spark on DataFibers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright (c) 2020, DataFibers all rights reserved.</copyright>
    <lastBuildDate>Wed, 28 Apr 2021 20:50:46 +0200</lastBuildDate>
    
	<atom:link href="https://datafibers-community.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Spark SQL in Depth</title>
      <link>https://datafibers-community.github.io/blog/2021/04/28/2021-04-28-spark-sql-in-depth/</link>
      <pubDate>Wed, 28 Apr 2021 20:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2021/04/28/2021-04-28-spark-sql-in-depth/</guid>
      <description>In this article, we&amp;rsquo;ll look at how Spark SQL working on data queries in depth.
Checking Execution Plan Data Preparing create database if not exists test; create table if not exists test.t_name (name string); insert into test.t_name values (&#39;test1&#39;),(&#39;test2&#39;),(&#39;test3&#39;);  Test Code Preparing Below Scala code is used with testing with blocking at the standard input at the end. In this case, we can see more details from Spark WebUI.</description>
    </item>
    
    <item>
      <title>Apache Spark 3.1.1 Released :)</title>
      <link>https://datafibers-community.github.io/blog/2021/03/10/2021-03-10-spark-3.3.1-released/</link>
      <pubDate>Wed, 10 Mar 2021 20:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2021/03/10/2021-03-10-spark-3.3.1-released/</guid>
      <description>Apache Spark 3.1.1 is released on March 2, 2021. It is milestone release for Spark in 2021. This version of spark keeps making it more efficient and stable. Below are highlighted new features and changes.
 Python usability ANSI SQL compliance Query optimization enhancements Shuffle hash join improvements History Server support of structured streaming  Project Zen Project Zen was initiated in this release to improve PySpark’s usability in these three ways:</description>
    </item>
    
    <item>
      <title>Spark SQL Read/Write HBase</title>
      <link>https://datafibers-community.github.io/blog/2020/01/01/2020-01-19-spark-sql-read-write-hbase/</link>
      <pubDate>Wed, 01 Jan 2020 20:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2020/01/01/2020-01-19-spark-sql-read-write-hbase/</guid>
      <description>Apache Spark and Apache HBase are very commonly used big data frameworks. In many senarios, we need to use Spark to query and analyze the big volumn of data in HBase. Spark has wider support to read data as dataset from many kinds of data source. To read from HBase, Spark provides TableInputFormat, which as following disadvantages.
 There is only on scan triggerred in each task to read from HBase TableInputFormat does not support BulkGet Cannot leverage the optimization from Spark SQL catalyst  Considering the above points above, there is another choice by using Hortonworks/Cloudera Apache Spark—Apache HBase Connector short for (SHC).</description>
    </item>
    
    <item>
      <title>Naive Bayes Algorithm</title>
      <link>https://datafibers-community.github.io/blog/2018/03/10/2018-03-10-ml-naive-bayes/</link>
      <pubDate>Sat, 10 Mar 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/03/10/2018-03-10-ml-naive-bayes/</guid>
      <description>Background It would be difficult and practically impossible to classify a web page, a document, an email or any other lengthy text notes manually. This is where Naïve Bayes Classifier machine learning algorithm comes to the rescue. A classifier is a function that allocates a population’s element value from one of the available categories. For instance, Spam Filtering is a popular application of Naïve Bayes algorithm. Spam filter here, is a classifier that assigns a label Spam or Not Spam to all the emails.</description>
    </item>
    
    <item>
      <title>Hive Get the Max/Min</title>
      <link>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</link>
      <pubDate>Fri, 02 Feb 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</guid>
      <description>Most of time, we need to find the max or min value of particular columns as well as other columns. For example, we have following employee table.
DESC employee; +---------------+------------------------------+----------+--+ | col_name | data_type | comment | +---------------+------------------------------+----------+--+ | name | string | | | work_place | array&amp;lt;string&amp;gt; | | | gender_age | struct&amp;lt;gender:string,age:int&amp;gt;| | | skills_score | map&amp;lt;string,int&amp;gt; | | | depart_title | map&amp;lt;string,array&amp;lt;string&amp;gt;&amp;gt; | | +---------------+------------------------------+----------+--+ 5 rows selected (0.</description>
    </item>
    
    <item>
      <title>Big Data Books Reviews</title>
      <link>https://datafibers-community.github.io/blog/2018/01/10/2018-01-10-reviews-big-data/</link>
      <pubDate>Wed, 10 Jan 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/01/10/2018-01-10-reviews-big-data/</guid>
      <description>Learning Spark SQL  Level Ent.
 Level Mid.
 Level Adv.  Published in Sep. 2017. Start reading it.

Learning Apache Flink  Level Ent. Level Mid.  There are very few books about Apache Flink. Besides offical document, this is a good one for people who wants to know Flink quicker. This book, published in the earlier of 2017, covers most of core topics for Flink with examples.</description>
    </item>
    
    <item>
      <title>Simplify Big Data Streaming</title>
      <link>https://datafibers-community.github.io/blog/2017/07/20/2017-07-20-df-meetup-summer/</link>
      <pubDate>Thu, 20 Jul 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/07/20/2017-07-20-df-meetup-summer/</guid>
      <description>Here is our free training offered during 2017 summer meetup in Toronto, Canada.</description>
    </item>
    
    <item>
      <title>Spark Word Count Tutorial</title>
      <link>https://datafibers-community.github.io/blog/2017/07/01/2017-07-01-spark-word-count/</link>
      <pubDate>Sat, 01 Jul 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/07/01/2017-07-01-spark-word-count/</guid>
      <description>It is quite often to setup Apache Spark development environment through IDE. Since I do not cover much setup IDE details in my Spark course, I am here to give detail steps for developing the well known Spark word count example using scala API in Eclipse.
Environment  Apache Spark v1.6 Scala 2.10.4 Eclipse Scala IDE  Download Software Needed  Download the proper scala version and install it Download the Eclipse scala IDE from above link  Create A Scala Project  Open Scala Eclipse IDE.</description>
    </item>
    
    <item>
      <title>One Platform Initatives for Spark</title>
      <link>https://datafibers-community.github.io/blog/2017/06/24/2017-06-24-cloudera-launches-one-platform-initatives-to-advance-spark---copy/</link>
      <pubDate>Sat, 24 Jun 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/06/24/2017-06-24-cloudera-launches-one-platform-initatives-to-advance-spark---copy/</guid>
      <description>In the early of this September, the Chief Strategy Offer of Cloudera Mike Olson has announced that the next important initiatives for Couldera - One Platform to advance their investment on Apache Spark.
The Spark is originally invented by few guys who started up the Databrick. Later, Spark catches most attention from big data communities and companies by its high-performance in-memory computing framework, which can run on top of Hadoop Yarn.</description>
    </item>
    
  </channel>
</rss>