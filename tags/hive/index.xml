<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hive on DataFibers</title>
    <link>https://datafibers-community.github.io/tags/hive/</link>
    <description>Recent content in Hive on DataFibers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 May 2018 13:50:46 +0200</lastBuildDate>
    
	<atom:link href="https://datafibers-community.github.io/tags/hive/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Run Hive 1 and 2 Together</title>
      <link>https://datafibers-community.github.io/blog/2018/05/01/2018-05-01-hive-1-and-2-setup/</link>
      <pubDate>Tue, 01 May 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/05/01/2018-05-01-hive-1-and-2-setup/</guid>
      <description>Most of time, we need to generate a unique identifier column for dataframe. There are couple of ways doing that as follows.
https://stackoverflow.com/questions/35705038/how-do-i-add-an-persistent-column-of-row-ids-to-spark-dataframe
Solution 1 The most frequent way of doing it is to to firstly find the MAX of age in each SEX group and do SELF JOIN by matching SEX and the MAX age as follows. This will create two stages of jobs and NOT efficient.
&amp;gt; SELECT employee.</description>
    </item>
    
    <item>
      <title>Hive Get the Max/Min</title>
      <link>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</link>
      <pubDate>Fri, 02 Feb 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</guid>
      <description>Most of time, we need to find the max or min value of particular columns as well as other columns. For example, we have following employee table.
DESC employee; +---------------+------------------------------+----------+--+ | col_name | data_type | comment | +---------------+------------------------------+----------+--+ | name | string | | | work_place | array&amp;lt;string&amp;gt; | | | gender_age | struct&amp;lt;gender:string,age:int&amp;gt;| | | skills_score | map&amp;lt;string,int&amp;gt; | | | depart_title | map&amp;lt;string,array&amp;lt;string&amp;gt;&amp;gt; | | +---------------+------------------------------+----------+--+ 5 rows selected (0.</description>
    </item>
    
    <item>
      <title>Big Data Books Reviews</title>
      <link>https://datafibers-community.github.io/blog/2018/01/10/2018-01-10-reviews-big-data/</link>
      <pubDate>Wed, 10 Jan 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/01/10/2018-01-10-reviews-big-data/</guid>
      <description>Learning Spark SQL  Level Ent.
 Level Mid.
 Level Adv.  Published in Sep. 2017. Start reading it.

Learning Apache Flink  Level Ent. Level Mid.  There are very few books about Apache Flink. Besides offical document, this is a good one for people who wants to know Flink quicker. This book, published in the earlier of 2017, covers most of core topics for Flink with examples.</description>
    </item>
    
    <item>
      <title>Hive RowID Generation</title>
      <link>https://datafibers-community.github.io/blog/2017/11/02/2017-11-02-hive-rowid-generation/</link>
      <pubDate>Thu, 02 Nov 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/11/02/2017-11-02-hive-rowid-generation/</guid>
      <description>Introduction It is quite often that we need a unique identifier for each single rows in the Apache Hive tables. This is quite useful when you need such columns as surrogate keys in data warehouse, as the primary key for data or use as system nature keys. There are following ways of doing that in Hive.
ROW_NUMBER() Hive have a couple of internal functions to achieve this. ROW_NUMBER function, which can generate row number for each partition of data.</description>
    </item>
    
  </channel>
</rss>