<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hive on DataFibers</title>
    <link>https://datafibers-community.github.io/tags/hive/</link>
    <description>Recent content in Hive on DataFibers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright (c) 2020, DataFibers all rights reserved.</copyright>
    <lastBuildDate>Wed, 30 May 2018 13:50:46 +0200</lastBuildDate>
    
	<atom:link href="https://datafibers-community.github.io/tags/hive/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Run Hive 1 and 2 Together</title>
      <link>https://datafibers-community.github.io/blog/2018/05/30/2018-05-30-hive-1-and-2-setup/</link>
      <pubDate>Wed, 30 May 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/05/30/2018-05-30-hive-1-and-2-setup/</guid>
      <description>Overview The latest HDP 2.6.x has both Hive version 1 and 2 installed together. However, it does not allow user to run hive version to command directly, but only use beeline. The lab_dev repository here provides an demo virtual box image to have both Hive version configured properly.
Conf. Changes The trick thing to make both hive version working is do not add any setting in the .profile anymore. See below, I comments out all pervious hive settings.</description>
    </item>
    
    <item>
      <title>Hive Get the Max/Min</title>
      <link>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</link>
      <pubDate>Fri, 02 Feb 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</guid>
      <description>Most of time, we need to find the max or min value of particular columns as well as other columns. For example, we have following employee table.
DESC employee; +---------------+------------------------------+----------+--+ | col_name | data_type | comment | +---------------+------------------------------+----------+--+ | name | string | | | work_place | array&amp;lt;string&amp;gt; | | | gender_age | struct&amp;lt;gender:string,age:int&amp;gt;| | | skills_score | map&amp;lt;string,int&amp;gt; | | | depart_title | map&amp;lt;string,array&amp;lt;string&amp;gt;&amp;gt; | | +---------------+------------------------------+----------+--+ 5 rows selected (0.</description>
    </item>
    
    <item>
      <title>Big Data Books Reviews</title>
      <link>https://datafibers-community.github.io/blog/2018/01/10/2018-01-10-reviews-big-data/</link>
      <pubDate>Wed, 10 Jan 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/01/10/2018-01-10-reviews-big-data/</guid>
      <description>Learning Spark SQL  Level Ent.
 Level Mid.
 Level Adv.  Published in Sep. 2017. Start reading it.

Learning Apache Flink  Level Ent. Level Mid.  There are very few books about Apache Flink. Besides offical document, this is a good one for people who wants to know Flink quicker. This book, published in the earlier of 2017, covers most of core topics for Flink with examples.</description>
    </item>
    
    <item>
      <title>Hive RowID Generation</title>
      <link>https://datafibers-community.github.io/blog/2017/11/02/2017-11-02-hive-rowid-generation/</link>
      <pubDate>Thu, 02 Nov 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/11/02/2017-11-02-hive-rowid-generation/</guid>
      <description>Introduction It is quite often that we need a unique identifier for each single rows in the Apache Hive tables. This is quite useful when you need such columns as surrogate keys in data warehouse, as the primary key for data or use as system nature keys. There are following ways of doing that in Hive.
ROW_NUMBER() Hive have a couple of internal functions to achieve this. ROW_NUMBER function, which can generate row number for each partition of data.</description>
    </item>
    
  </channel>
</rss>