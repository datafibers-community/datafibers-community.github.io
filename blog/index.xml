<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on DataFibers</title>
    <link>https://datafibers-community.github.io/blog/</link>
    <description>Recent content in Blogs on DataFibers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright (c) 2018, DataFibers all rights reserved.</copyright>
    <lastBuildDate>Sat, 10 Mar 2018 13:50:46 +0200</lastBuildDate>
    
	<atom:link href="https://datafibers-community.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Naive Bayes Algorithm</title>
      <link>https://datafibers-community.github.io/blog/2018/03/10/2018-03-10-ml-naive-bayes/</link>
      <pubDate>Sat, 10 Mar 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/03/10/2018-03-10-ml-naive-bayes/</guid>
      <description>Background It would be difficult and practically impossible to classify a web page, a document, an email or any other lengthy text notes manually. This is where Naïve Bayes Classifier machine learning algorithm comes to the rescue. A classifier is a function that allocates a population’s element value from one of the available categories. For instance, Spam Filtering is a popular application of Naïve Bayes algorithm. Spam filter here, is a classifier that assigns a label Spam or Not Spam to all the emails.</description>
    </item>
    
    <item>
      <title>ML Overview</title>
      <link>https://datafibers-community.github.io/blog/2018/03/02/2018-03-02-ml-overview/</link>
      <pubDate>Fri, 02 Mar 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/03/02/2018-03-02-ml-overview/</guid>
      <description>Background Machine learning is a field of computer science that gives computer systems the ability to &amp;ldquo;learn&amp;rdquo; with data, without being explicitly programmed. Machine learning can be broken down into three broad categories: Recommender, Classification, Clustering.
 Recommender—Recommender systems suggest items based on past behavior or interest. These items can be other users in a social network, or products and services in retail websites. There are some algorithm like Pearson correlation and euclidean distance.</description>
    </item>
    
    <item>
      <title>Hive Get the Max/Min</title>
      <link>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</link>
      <pubDate>Fri, 02 Feb 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</guid>
      <description>Most of time, we need to find the max or min value of particular columns as well as other columns. For example, we have following employee table.
&amp;gt; SELECT name,sex_age.sex AS sex,sex_age.age AS age FROM employee; +----------+---------+------+ | name | sex | age | +----------+---------+------+ | Michael | Male | 30 | | Will | Male | 35 | | Shelley | Female | 27 | | Lucy | Female | 57 | | Steven | Male | 30 | +----------+---------+------+ 5 rows selected (75.</description>
    </item>
    
    <item>
      <title>Big Data Books Reviews</title>
      <link>https://datafibers-community.github.io/blog/2018/01/10/2018-01-10-reviews-big-data/</link>
      <pubDate>Wed, 10 Jan 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/01/10/2018-01-10-reviews-big-data/</guid>
      <description>Learning Spark SQL  Level Ent.
 Level Mid.
 Level Adv.  Published in Sep. 2017. Start reading it.

Learning Apache Flink  Level Ent. Level Mid.  There are very few books about Apache Flink. Besides offical document, this is a good one for people who wants to know Flink quicker. This book, published in the earlier of 2017, covers most of core topics for Flink with examples.</description>
    </item>
    
    <item>
      <title>2017 Winter Release</title>
      <link>https://datafibers-community.github.io/blog/2017/12/22/2017-12-22-df-winter-release/</link>
      <pubDate>Fri, 22 Dec 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/12/22/2017-12-22-df-winter-release/</guid>
      <description>Summary Before christmas, DataFibers has completed the winter release, which has more than 40+ changes requests applied. In this release, DataFibers is featured with first demo combined both data landing and transforming in real time with new web interface. In addition, the preview version of batch processing (by spark) is ready.
Details Below is the list of key changes in this release.
 New Web admin UI released using ReactJs based AOR.</description>
    </item>
    
    <item>
      <title>Hive RowID Generation</title>
      <link>https://datafibers-community.github.io/blog/2017/11/02/2017-11-02-hive-rowid-generation/</link>
      <pubDate>Thu, 02 Nov 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/11/02/2017-11-02-hive-rowid-generation/</guid>
      <description>Introduction It is quite often that we need a unique identifier for each single rows in the Apache Hive tables. This is quite useful when you need such columns as surrogate keys in data warehouse, as the primary key for data or use as system nature keys. There are following ways of doing that in Hive.
ROW_NUMBER() Hive have a couple of internal functions to achieve this. ROW_NUMBER function, which can generate row number for each partition of data.</description>
    </item>
    
    <item>
      <title>GIT Tips</title>
      <link>https://datafibers-community.github.io/blog/2017/11/01/2017-11-01-git-tips/</link>
      <pubDate>Wed, 01 Nov 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/11/01/2017-11-01-git-tips/</guid>
      <description>1. Git Cheat Sheet 2. Check in Git Modified But Untracked Content Recently, I migrate this site to Hexo. I download the theme from github to the Hexo project folder. I also keep the source code in the github in case I lost the source code. However, when I run the git add . and git status. It shows error messages saying the theme folder is not tracked content. Most time, I did not check the git status - bad habit.</description>
    </item>
    
    <item>
      <title>Apache Kafka Overview</title>
      <link>https://datafibers-community.github.io/blog/2017/10/05/2017-10-05-apache-kafka-overview/</link>
      <pubDate>Thu, 05 Oct 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/10/05/2017-10-05-apache-kafka-overview/</guid>
      <description>The big data processing started by focusing on the batch processing. Distributed data storage and querying tools like MapReduce, Hive, and Pig were all designed to process data in batches rather than continuously. Recently enterprises have discovered the power of analyzing and processing data and events as they happen instead of batches. Most traditional messaging systems, such as RabbitMq, neither scale up to handle big data in realtime nor use friendly with big data ecosystem.</description>
    </item>
    
    <item>
      <title>Scala Apply Method</title>
      <link>https://datafibers-community.github.io/blog/2017/09/15/2017-09-15-scala-apply-method/</link>
      <pubDate>Fri, 15 Sep 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/09/15/2017-09-15-scala-apply-method/</guid>
      <description>The apply methods in scala has a nice syntactic sugar. It allows us to define semantics like java array access for an arbitrary class.
For example, we create a class of RiceCooker and its method cook to cook rice. Whenever we need to cook rice, we could call this method.
class RiceCooker { def cook(cup_of_rice: Rice) = { cup_of_rice.isDone = true cup_of_rice } } val my_rice_cooker: RiceCooker = new RiceCooker() my_rice_cooker.</description>
    </item>
    
    <item>
      <title>2017 Summer Release</title>
      <link>https://datafibers-community.github.io/blog/2017/08/31/2017-08-31-df-summer-release/</link>
      <pubDate>Thu, 31 Aug 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/08/31/2017-08-31-df-summer-release/</guid>
      <description>Summary A little bit data, but DataFibers has completed the summer release of 2017 about right time. In this release, we have applied 30+ changes requests. In this release, DataFibers is featured with a preview of new web interface. In addition, couple of connectors are added/updated to preparing the demo later.
Details Below is the list of key changes in this release.
 Support Flink Table API and SQL API support Flink upgrade to v1.</description>
    </item>
    
    <item>
      <title>Simplify Big Data Streaming</title>
      <link>https://datafibers-community.github.io/blog/2017/07/20/2017-07-20-df-meetup-summer/</link>
      <pubDate>Thu, 20 Jul 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/07/20/2017-07-20-df-meetup-summer/</guid>
      <description>Here is our free training offered during 2017 summer meetup in Toronto, Canada.</description>
    </item>
    
    <item>
      <title>Spark Word Count Tutorial</title>
      <link>https://datafibers-community.github.io/blog/2017/07/01/2017-07-01-spark-word-count/</link>
      <pubDate>Sat, 01 Jul 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/07/01/2017-07-01-spark-word-count/</guid>
      <description>It is quite often to setup Apache Spark development environment through IDE. Since I do not cover much setup IDE details in my Spark course, I am here to give detail steps for developing the well known Spark word count example using scala API in Eclipse.
Environment  Apache Spark v1.6 Scala 2.10.4 Eclipse Scala IDE  Download Software Needed  Download the proper scala version and install it Download the Eclipse scala IDE from above link  Create A Scala Project  Open Scala Eclipse IDE.</description>
    </item>
    
    <item>
      <title>One Platform Initatives for Spark</title>
      <link>https://datafibers-community.github.io/blog/2017/06/24/2017-06-24-cloudera-launches-one-platform-initatives-to-advance-spark---copy/</link>
      <pubDate>Sat, 24 Jun 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/06/24/2017-06-24-cloudera-launches-one-platform-initatives-to-advance-spark---copy/</guid>
      <description>In the early of this September, the Chief Strategy Offer of Cloudera Mike Olson has announced that the next important initiatives for Couldera - One Platform to advance their investment on Apache Spark.
The Spark is originally invented by few guys who started up the Databrick. Later, Spark catches most attention from big data communities and companies by its high-performance in-memory computing framework, which can run on top of Hadoop Yarn.</description>
    </item>
    
    <item>
      <title>Constructor - Scala vs. Java</title>
      <link>https://datafibers-community.github.io/blog/2017/05/01/2017-05-01-scala-and-java-constructors/</link>
      <pubDate>Mon, 01 May 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/05/01/2017-05-01-scala-and-java-constructors/</guid>
      <description>1. Constructor With Parameters Java Code
public class Foo() { public Bar bar; public Foo(Bar bar) { this.bar = bar; } }  Scala Code
class Foo(val bar:Bar)  2. Constructor With Private Attribute Java Code
public class Foo() { private final Bar bar; public Foo(Bar bar) { this.bar = bar; } }  Scala Code
class Foo(private val bar:Bar)  3. Call Super Constructor Java Code
public class Foo() extends SuperFoo { public Foo(Bar bar) { super(bar); } }  Scala Code</description>
    </item>
    
    <item>
      <title>2016 Winter Release</title>
      <link>https://datafibers-community.github.io/blog/2016/12/29/2016-12-29-df-winter-release/</link>
      <pubDate>Thu, 29 Dec 2016 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2016/12/29/2016-12-29-df-winter-release/</guid>
      <description>Summary Before new year, DataFibers has completed the winter release of 2016, which has more than 20+ changes requests applied. In this release, DataFibers is featured with new api document and landing pages. In addition, the preview version of stream processing (by flink) is ready.
Details Below is the list of key changes in this release.
 Integrated REST API Document to the DF Application
 Added landing welcome page</description>
    </item>
    
    <item>
      <title>When to Disable Speculative Execution</title>
      <link>https://datafibers-community.github.io/blog/2016/11/22/2016-11-22-when-to-disable-speculative-execution/</link>
      <pubDate>Tue, 22 Nov 2016 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2016/11/22/2016-11-22-when-to-disable-speculative-execution/</guid>
      <description>Backgrounds This is the link from WikiMedia about what’s Speculative Execution. In Hadoop, the following parameters string are for this settings. And, they are true by default.
mapred.map.tasks.speculative.execution mapred.reduce.tasks.speculative.execution  When to Disable Most time, it helps. However, I am here to collect some scenario when we do not need it.
 Of course, when ever your cluster really in shortage of resource or for the purpose of experiment, we can disable them by setting them to false since “SE” really a big resource consumer It is generally advisable to turn off ”SE” for mapred jobs that use HBase as a source.</description>
    </item>
    
  </channel>
</rss>