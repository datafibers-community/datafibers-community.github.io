<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on DataFibers</title>
    <link>https://datafibers-community.github.io/blog/</link>
    <description>Recent content in Blogs on DataFibers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright (c) 2020, DataFibers all rights reserved.</copyright>
    <lastBuildDate>Sat, 01 Jul 2023 10:17:46 +0200</lastBuildDate>
    
	<atom:link href="https://datafibers-community.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Leading Cloud Tech. Stack Comparison</title>
      <link>https://datafibers-community.github.io/blog/2023/07/01/2023-07-31-leading-cloud-technology-stack/</link>
      <pubDate>Sat, 01 Jul 2023 10:17:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2023/07/01/2023-07-31-leading-cloud-technology-stack/</guid>
      <description>Introduction: In today&amp;rsquo;s digital era, businesses are increasingly adopting cloud computing to scale their operations, enhance flexibility, and reduce costs. Among the major cloud service providers, Amazon Web Services (AWS), Microsoft Azure, Google Cloud, and Oracle Cloud have emerged as dominant players in the market. Each offers a comprehensive cloud technology stack tailored to meet different business needs. In this blog, we&amp;rsquo;ll conduct a thorough comparison of these leading cloud technology stacks to help you make an informed decision when choosing the best-fit cloud provider for your organization.</description>
    </item>
    
    <item>
      <title>Embracing Kubernetes, Goodbye Spring Cloud</title>
      <link>https://datafibers-community.github.io/blog/2021/06/19/2021-06-19-embracing-kubernetes-goodbye-spring-cloud-copy/</link>
      <pubDate>Sat, 19 Jun 2021 10:17:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2021/06/19/2021-06-19-embracing-kubernetes-goodbye-spring-cloud-copy/</guid>
      <description>I believe many developers, after familiarizing themselves with microservices, realized that they thought they had successfully built a microservices architecture empired with Spring Cloud. But after the popular of kubernetes (K8S), they were curious and exciting of creating the cloud native microservices serivces.
The Era of Spring Boot and Cloud In October 2012, Mike Youngstrom created a feature request in Spring Jira to support a containerless web application architecture in the Spring Framework.</description>
    </item>
    
    <item>
      <title>Spark SQL in Depth</title>
      <link>https://datafibers-community.github.io/blog/2021/04/28/2021-04-28-spark-sql-in-depth-copy/</link>
      <pubDate>Wed, 28 Apr 2021 20:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2021/04/28/2021-04-28-spark-sql-in-depth-copy/</guid>
      <description>In this article, we&amp;rsquo;ll look at how Spark SQL working on data queries in depth.
Checking Execution Plan Data Preparing create database if not exists test; create table if not exists test.t_name (name string); insert into test.t_name values (&#39;test1&#39;),(&#39;test2&#39;),(&#39;test3&#39;);  Test Code Preparing Below Scala code is used with testing with blocking at the standard input at the end. In this case, we can see more details from Spark WebUI.</description>
    </item>
    
    <item>
      <title>Apache Spark 3.1.1 Released :)</title>
      <link>https://datafibers-community.github.io/blog/2021/03/10/2021-03-10-spark-3.3.1-released/</link>
      <pubDate>Wed, 10 Mar 2021 20:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2021/03/10/2021-03-10-spark-3.3.1-released/</guid>
      <description>Apache Spark 3.1.1 is released on March 2, 2021. It is milestone release for Spark in 2021. This version of spark keeps making it more efficient and stable. Below are highlighted new features and changes.
 Python usability ANSI SQL compliance Query optimization enhancements Shuffle hash join improvements History Server support of structured streaming  Project Zen Project Zen was initiated in this release to improve PySpark’s usability in these three ways:</description>
    </item>
    
    <item>
      <title>Apache Superset:)</title>
      <link>https://datafibers-community.github.io/blog/2021/01/22/2021-01-22-apache-superset/</link>
      <pubDate>Fri, 22 Jan 2021 20:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2021/01/22/2021-01-22-apache-superset/</guid>
      <description>On January 21, 2021, Apache&amp;rsquo;s official announced that Apache® Superset™ has become a top-level project. Apache® Superset™ is a modern big data exploration and visualization platform that allows users to build dashboards quickly and easily using a simple code-free visualization builder and the most advanced SQL editor. The project was launched on Airbnb in 2015 and entered the Apache incubator in May 2017. Apache Superset is a big data-related BI visualization tool.</description>
    </item>
    
    <item>
      <title>Spark SQL Read/Write HBase</title>
      <link>https://datafibers-community.github.io/blog/2020/01/01/2020-01-19-spark-sql-read-write-hbase/</link>
      <pubDate>Wed, 01 Jan 2020 20:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2020/01/01/2020-01-19-spark-sql-read-write-hbase/</guid>
      <description>Apache Spark and Apache HBase are very commonly used big data frameworks. In many senarios, we need to use Spark to query and analyze the big volumn of data in HBase. Spark has wider support to read data as dataset from many kinds of data source. To read from HBase, Spark provides TableInputFormat, which as following disadvantages.
 There is only on scan triggerred in each task to read from HBase TableInputFormat does not support BulkGet Cannot leverage the optimization from Spark SQL catalyst  Considering the above points above, there is another choice by using Hortonworks/Cloudera Apache Spark—Apache HBase Connector short for (SHC).</description>
    </item>
    
    <item>
      <title>Apache Airflow Overview</title>
      <link>https://datafibers-community.github.io/blog/2019/12/15/2019-12-15-apache-airflow-overview/</link>
      <pubDate>Sun, 15 Dec 2019 19:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2019/12/15/2019-12-15-apache-airflow-overview/</guid>
      <description>What is Airflow? Airflow is a platform to programmaticaly author, schedule and monitor workflows or data pipelines. It composes Directed Acyclic Graph (DAG) with multiple tasks which can be executed independently. The Airflow scheduler executes the tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.</description>
    </item>
    
    <item>
      <title>Apache Kafka Producers</title>
      <link>https://datafibers-community.github.io/blog/2019/11/02/2019-11-02-apache-kafka-producers/</link>
      <pubDate>Sat, 02 Nov 2019 19:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2019/11/02/2019-11-02-apache-kafka-producers/</guid>
      <description>Kafka producers send records to topics. The records are sometimes referred to as messages.
The producer picks which partition to send a record to per topic. The producer can send records round-robin. The producer could implement priority systems based on sending records to certain partitions based on the priority of the record. Generally speaking, producers send records to a partition based on the record’s key. The default partitioner for Java uses a hash of the record’s key to choose the partition or uses a round-robin strategy if the record has no key.</description>
    </item>
    
    <item>
      <title>Flink Windows Explained</title>
      <link>https://datafibers-community.github.io/blog/2019/10/05/2019-10-05-flink-windows-explained/</link>
      <pubDate>Sat, 05 Oct 2019 19:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2019/10/05/2019-10-05-flink-windows-explained/</guid>
      <description>Overview Apache Flink supports data analysis over specific ranges in terms of windows. It supports two ways to create windows, time and count. Time window defines windows by specific time range. Count window defines windows by specifc number of envents.
In addition, there are two windows time attributes.
 size: how long the window itsef last interval: how long between windows  Whenever the window size = interval, this type of windows are called tumbling-window.</description>
    </item>
    
    <item>
      <title>The Complete SQL Tuning</title>
      <link>https://datafibers-community.github.io/blog/2019/09/02/2019-09-02-complete-sql-tuning/</link>
      <pubDate>Mon, 02 Sep 2019 14:53:46 +0300</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2019/09/02/2019-09-02-complete-sql-tuning/</guid>
      <description>The most practice comes for MySQL server, but it applies to other relational database as well.
 Aviod full table scan and try to create index on the columns used after where or order by.
 Aviod check null after where clause. You set set null as default value when creating tables. However, mostly we should use not null value or use special value, such as 0 or -1 for instead.</description>
    </item>
    
    <item>
      <title>Big Data Stack Compare</title>
      <link>https://datafibers-community.github.io/blog/2019/08/03/2019-08-03-big-data-stack-compare/</link>
      <pubDate>Sat, 03 Aug 2019 14:53:46 +0300</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2019/08/03/2019-08-03-big-data-stack-compare/</guid>
      <description>1. Batch Processing ETL + ELK ELK stands for Elastisearch, Logstash, Kibana and is a powerful tool for real-time logs analysis. Performance depends on the amount of RAM for the cluster. If the full index is in RAM search will have close to zero latency. This solution also supports storing similar information in one cluster to enhance speed. ELK can be hard to maintain if the index is growing big, but scaling is achieved by adding new nodes.</description>
    </item>
    
    <item>
      <title>What Does Big Data Engineer Do?</title>
      <link>https://datafibers-community.github.io/blog/2019/07/01/2019-07-01-what-does-big-data-engineer-do/</link>
      <pubDate>Mon, 01 Jul 2019 14:53:46 +0300</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2019/07/01/2019-07-01-what-does-big-data-engineer-do/</guid>
      <description>As the the big data has become more matured, data engineering has emerged as a separate and related role that works in concert with data scientists. Big data engineer or data engineer becomes more and more important role in big data orgnization. This role is quite like the ETL developer role in the data warehouse or database developer role in database development. However, it more focus on the senario in Applied Big Data.</description>
    </item>
    
    <item>
      <title>All About Big Data Interviews</title>
      <link>https://datafibers-community.github.io/blog/2019/06/02/2019-06-02-all-about-big-data-interviews/</link>
      <pubDate>Sun, 02 Jun 2019 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2019/06/02/2019-06-02-all-about-big-data-interviews/</guid>
      <description>Quite often, we got chances to go for big data interviews or interview some candidates. Most of time, we could add some short questions in addition to the white board coding. Here, we collect a few aspects of areas we can focus during the interview or prepaing the coming interviews.
Concept 1. What&amp;rsquo;s the reason to use Dequeue instead of Stack in Java. Dequeue has the ability to use streams convert to list with keeping LIFO concept applied while stack does not.</description>
    </item>
    
    <item>
      <title>Use Redish Lock for SecKill</title>
      <link>https://datafibers-community.github.io/blog/2019/05/20/2019-05-20-use-redis-lock-for-seckill/</link>
      <pubDate>Mon, 20 May 2019 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2019/05/20/2019-05-20-use-redis-lock-for-seckill/</guid>
      <description>What is Seckill? When associated with online shopping, &amp;ldquo;seckill&amp;rdquo; refers to the quick sell out of newly-advertised goods. If you look at the transaction record, you will find that each of the transactions is made in seconds. It sounds inconceivable but is the naked truth. This is called &amp;ldquo;seckill&amp;rdquo;.
A typical system for seckill has following features. * A large number of users will be shopping at the same time during the quick sell, and the web site traffic increses dramatically.</description>
    </item>
    
    <item>
      <title>Apache Kafka Consumers</title>
      <link>https://datafibers-community.github.io/blog/2018/08/04/2018-08-04-apache-kafka-consumers/</link>
      <pubDate>Sat, 04 Aug 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/08/04/2018-08-04-apache-kafka-consumers/</guid>
      <description>Kafka consumer is what we use quite often to read data from Kafka. Here, we use this article to explain some key concepts and topics regarding to consumer architecture in Kafka.
Consumer Groups We can always group consumers into a consumer group by use case or function of the group. One consumer group might be responsible for delivering records to high-speed, in-memory microservices while another consumer group is streaming those same records to Hadoop.</description>
    </item>
    
    <item>
      <title>NoSQL Overview</title>
      <link>https://datafibers-community.github.io/blog/2018/06/17/2018-06-17-nosql-overview/</link>
      <pubDate>Sun, 17 Jun 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/06/17/2018-06-17-nosql-overview/</guid>
      <description>Overview NoSQL (NoSQL = Not Only SQL) means &amp;ldquo;not just SQL&amp;rdquo;. Modern computing systems generate a huge amount of data every day on the network. A large part of these data are handled by relational database management systems (RDBMSs). Its matured relational theory foundation makes data modeling and application programming easier. However, with the wave of informationization and the rise of the Internet, traditional RDBMSs have started to experience problems in some paticular domain.</description>
    </item>
    
    <item>
      <title>Run Hive 1 and 2 Together</title>
      <link>https://datafibers-community.github.io/blog/2018/05/30/2018-05-30-hive-1-and-2-setup/</link>
      <pubDate>Wed, 30 May 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/05/30/2018-05-30-hive-1-and-2-setup/</guid>
      <description>Overview The latest HDP 2.6.x has both Hive version 1 and 2 installed together. However, it does not allow user to run hive version to command directly, but only use beeline. The lab_dev repository here provides an demo virtual box image to have both Hive version configured properly.
Conf. Changes The trick thing to make both hive version working is do not add any setting in the .profile anymore. See below, I comments out all pervious hive settings.</description>
    </item>
    
    <item>
      <title>HBase Shell Reference</title>
      <link>https://datafibers-community.github.io/blog/2018/04/28/2018-04-28-hbase-shell_example/</link>
      <pubDate>Sat, 28 Apr 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/04/28/2018-04-28-hbase-shell_example/</guid>
      <description>We use this place to collect commonly used HBase shell command for reference. HBase shell is an HBase extensible jruby-based (JIRB) shell to execute some commands(each command represents one functionality) in HBase. HBase shell commands are mainly categorized into 6 parts as follows. Will keep adding more examples here.
1. General Information status Show cluster status. Can be &amp;lsquo;summary&amp;rsquo;, &amp;lsquo;simple&amp;rsquo;, or &amp;lsquo;detailed&amp;rsquo;. The default is &amp;lsquo;summary&amp;rsquo;.
hbase&amp;gt; status hbase&amp;gt; status &#39;simple&#39; hbase&amp;gt; status &#39;summary&#39; hbase&amp;gt; status &#39;detailed&#39;  version Output this HBase version.</description>
    </item>
    
    <item>
      <title>Naive Bayes Algorithm</title>
      <link>https://datafibers-community.github.io/blog/2018/03/10/2018-03-10-ml-naive-bayes/</link>
      <pubDate>Sat, 10 Mar 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/03/10/2018-03-10-ml-naive-bayes/</guid>
      <description>Background It would be difficult and practically impossible to classify a web page, a document, an email or any other lengthy text notes manually. This is where Naïve Bayes Classifier machine learning algorithm comes to the rescue. A classifier is a function that allocates a population’s element value from one of the available categories. For instance, Spam Filtering is a popular application of Naïve Bayes algorithm. Spam filter here, is a classifier that assigns a label Spam or Not Spam to all the emails.</description>
    </item>
    
    <item>
      <title>ML Overview</title>
      <link>https://datafibers-community.github.io/blog/2018/03/02/2018-03-02-ml-overview/</link>
      <pubDate>Fri, 02 Mar 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/03/02/2018-03-02-ml-overview/</guid>
      <description>Background Machine learning is a field of computer science that gives computer systems the ability to &amp;ldquo;learn&amp;rdquo; with data, without being explicitly programmed. Machine learning can be broken down into three broad categories: Recommender, Classification, Clustering.
 Recommender—Recommender systems suggest items based on past behavior or interest. These items can be other users in a social network, or products and services in retail websites. There are some algorithm like Pearson correlation and euclidean distance.</description>
    </item>
    
    <item>
      <title>Hive Get the Max/Min</title>
      <link>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</link>
      <pubDate>Fri, 02 Feb 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</guid>
      <description>Most of time, we need to find the max or min value of particular columns as well as other columns. For example, we have following employee table.
DESC employee; +---------------+------------------------------+----------+--+ | col_name | data_type | comment | +---------------+------------------------------+----------+--+ | name | string | | | work_place | array&amp;lt;string&amp;gt; | | | gender_age | struct&amp;lt;gender:string,age:int&amp;gt;| | | skills_score | map&amp;lt;string,int&amp;gt; | | | depart_title | map&amp;lt;string,array&amp;lt;string&amp;gt;&amp;gt; | | +---------------+------------------------------+----------+--+ 5 rows selected (0.</description>
    </item>
    
    <item>
      <title>Big Data Books Reviews</title>
      <link>https://datafibers-community.github.io/blog/2018/01/10/2018-01-10-reviews-big-data/</link>
      <pubDate>Wed, 10 Jan 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/01/10/2018-01-10-reviews-big-data/</guid>
      <description>Learning Spark SQL  Level Ent.
 Level Mid.
 Level Adv.  Published in Sep. 2017. Start reading it.

Learning Apache Flink  Level Ent. Level Mid.  There are very few books about Apache Flink. Besides offical document, this is a good one for people who wants to know Flink quicker. This book, published in the earlier of 2017, covers most of core topics for Flink with examples.</description>
    </item>
    
    <item>
      <title>2017 Winter Release</title>
      <link>https://datafibers-community.github.io/blog/2017/12/22/2017-12-22-df-winter-release/</link>
      <pubDate>Fri, 22 Dec 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/12/22/2017-12-22-df-winter-release/</guid>
      <description>Summary Before christmas, DataFibers has completed the winter release, which has more than 40+ changes requests applied. In this release, DataFibers is featured with first demo combined both data landing and transforming in real time with new web interface. In addition, the preview version of batch processing (by spark) is ready.
Details Below is the list of key changes in this release.
 New Web admin UI released using ReactJs based AOR.</description>
    </item>
    
    <item>
      <title>Hive RowID Generation</title>
      <link>https://datafibers-community.github.io/blog/2017/11/02/2017-11-02-hive-rowid-generation/</link>
      <pubDate>Thu, 02 Nov 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/11/02/2017-11-02-hive-rowid-generation/</guid>
      <description>Introduction It is quite often that we need a unique identifier for each single rows in the Apache Hive tables. This is quite useful when you need such columns as surrogate keys in data warehouse, as the primary key for data or use as system nature keys. There are following ways of doing that in Hive.
ROW_NUMBER() Hive have a couple of internal functions to achieve this. ROW_NUMBER function, which can generate row number for each partition of data.</description>
    </item>
    
    <item>
      <title>GIT Tips</title>
      <link>https://datafibers-community.github.io/blog/2017/11/01/2017-11-01-git-tips/</link>
      <pubDate>Wed, 01 Nov 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/11/01/2017-11-01-git-tips/</guid>
      <description>1. Git Cheat Sheet 2. Check in Git Modified But Untracked Content Recently, I migrate this site to Hexo. I download the theme from github to the Hexo project folder. I also keep the source code in the github in case I lost the source code. However, when I run the git add . and git status. It shows error messages saying the theme folder is not tracked content. Most time, I did not check the git status - bad habit.</description>
    </item>
    
    <item>
      <title>Apache Kafka Overview</title>
      <link>https://datafibers-community.github.io/blog/2017/10/05/2017-10-05-apache-kafka-overview/</link>
      <pubDate>Thu, 05 Oct 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/10/05/2017-10-05-apache-kafka-overview/</guid>
      <description>The big data processing started by focusing on the batch processing. Distributed data storage and querying tools like MapReduce, Hive, and Pig were all designed to process data in batches rather than continuously. Recently enterprises have discovered the power of analyzing and processing data and events as they happen instead of batches. Most traditional messaging systems, such as RabbitMq, neither scale up to handle big data in realtime nor use friendly with big data ecosystem.</description>
    </item>
    
    <item>
      <title>Scala Apply Method</title>
      <link>https://datafibers-community.github.io/blog/2017/09/15/2017-09-15-scala-apply-method/</link>
      <pubDate>Fri, 15 Sep 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/09/15/2017-09-15-scala-apply-method/</guid>
      <description>The apply methods in scala has a nice syntactic sugar. It allows us to define semantics like java array access for an arbitrary class.
For example, we create a class of RiceCooker and its method cook to cook rice. Whenever we need to cook rice, we could call this method.
class RiceCooker { def cook(cup_of_rice: Rice) = { cup_of_rice.isDone = true cup_of_rice } } val my_rice_cooker: RiceCooker = new RiceCooker() my_rice_cooker.</description>
    </item>
    
    <item>
      <title>2017 Summer Release</title>
      <link>https://datafibers-community.github.io/blog/2017/08/31/2017-08-31-df-summer-release/</link>
      <pubDate>Thu, 31 Aug 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/08/31/2017-08-31-df-summer-release/</guid>
      <description>Summary A little bit data, but DataFibers has completed the summer release of 2017 about right time. In this release, we have applied 30+ changes requests. In this release, DataFibers is featured with a preview of new web interface. In addition, couple of connectors are added/updated to preparing the demo later.
Details Below is the list of key changes in this release.
 Support Flink Table API and SQL API support Flink upgrade to v1.</description>
    </item>
    
    <item>
      <title>Simplify Big Data Streaming</title>
      <link>https://datafibers-community.github.io/blog/2017/07/20/2017-07-20-df-meetup-summer/</link>
      <pubDate>Thu, 20 Jul 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/07/20/2017-07-20-df-meetup-summer/</guid>
      <description>Here is our free training offered during 2017 summer meetup in Toronto, Canada.</description>
    </item>
    
    <item>
      <title>Spark Word Count Tutorial</title>
      <link>https://datafibers-community.github.io/blog/2017/07/01/2017-07-01-spark-word-count/</link>
      <pubDate>Sat, 01 Jul 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/07/01/2017-07-01-spark-word-count/</guid>
      <description>It is quite often to setup Apache Spark development environment through IDE. Since I do not cover much setup IDE details in my Spark course, I am here to give detail steps for developing the well known Spark word count example using scala API in Eclipse.
Environment  Apache Spark v1.6 Scala 2.10.4 Eclipse Scala IDE  Download Software Needed  Download the proper scala version and install it Download the Eclipse scala IDE from above link  Create A Scala Project  Open Scala Eclipse IDE.</description>
    </item>
    
    <item>
      <title>One Platform Initatives for Spark</title>
      <link>https://datafibers-community.github.io/blog/2017/06/24/2017-06-24-cloudera-launches-one-platform-initatives-to-advance-spark---copy/</link>
      <pubDate>Sat, 24 Jun 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/06/24/2017-06-24-cloudera-launches-one-platform-initatives-to-advance-spark---copy/</guid>
      <description>In the early of this September, the Chief Strategy Offer of Cloudera Mike Olson has announced that the next important initiatives for Couldera - One Platform to advance their investment on Apache Spark.
The Spark is originally invented by few guys who started up the Databrick. Later, Spark catches most attention from big data communities and companies by its high-performance in-memory computing framework, which can run on top of Hadoop Yarn.</description>
    </item>
    
    <item>
      <title>Constructor - Scala vs. Java</title>
      <link>https://datafibers-community.github.io/blog/2017/05/01/2017-05-01-scala-and-java-constructors/</link>
      <pubDate>Mon, 01 May 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/05/01/2017-05-01-scala-and-java-constructors/</guid>
      <description>1. Constructor With Parameters Java Code
public class Foo() { public Bar bar; public Foo(Bar bar) { this.bar = bar; } }  Scala Code
class Foo(val bar:Bar)  2. Constructor With Private Attribute Java Code
public class Foo() { private final Bar bar; public Foo(Bar bar) { this.bar = bar; } }  Scala Code
class Foo(private val bar:Bar)  3. Call Super Constructor Java Code
public class Foo() extends SuperFoo { public Foo(Bar bar) { super(bar); } }  Scala Code</description>
    </item>
    
    <item>
      <title>2016 Winter Release</title>
      <link>https://datafibers-community.github.io/blog/2016/12/29/2016-12-29-df-winter-release/</link>
      <pubDate>Thu, 29 Dec 2016 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2016/12/29/2016-12-29-df-winter-release/</guid>
      <description>Summary Before new year, DataFibers has completed the winter release of 2016, which has more than 20+ changes requests applied. In this release, DataFibers is featured with new api document and landing pages. In addition, the preview version of stream processing (by flink) is ready.
Details Below is the list of key changes in this release.
 Integrated REST API Document to the DF Application
 Added landing welcome page</description>
    </item>
    
    <item>
      <title>When to Disable Speculative Execution</title>
      <link>https://datafibers-community.github.io/blog/2016/11/22/2016-11-22-when-to-disable-speculative-execution/</link>
      <pubDate>Tue, 22 Nov 2016 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2016/11/22/2016-11-22-when-to-disable-speculative-execution/</guid>
      <description>Backgrounds This is the link from WikiMedia about what’s Speculative Execution. In Hadoop, the following parameters string are for this settings. And, they are true by default.
mapred.map.tasks.speculative.execution mapred.reduce.tasks.speculative.execution  When to Disable Most time, it helps. However, I am here to collect some scenario when we do not need it.
 Of course, when ever your cluster really in shortage of resource or for the purpose of experiment, we can disable them by setting them to false since “SE” really a big resource consumer It is generally advisable to turn off ”SE” for mapred jobs that use HBase as a source.</description>
    </item>
    
  </channel>
</rss>