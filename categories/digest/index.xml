<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Digest on DataFibers</title>
    <link>https://datafibers-community.github.io/categories/digest/</link>
    <description>Recent content in Digest on DataFibers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright (c) 2020, DataFibers all rights reserved.</copyright>
    <lastBuildDate>Sat, 19 Jun 2021 10:17:46 +0200</lastBuildDate>
    
	<atom:link href="https://datafibers-community.github.io/categories/digest/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Embracing Kubernetes, Goodbye Spring Cloud</title>
      <link>https://datafibers-community.github.io/blog/2021/06/19/2021-06-19-embracing-kubernetes-goodbye-spring-cloud/</link>
      <pubDate>Sat, 19 Jun 2021 10:17:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2021/06/19/2021-06-19-embracing-kubernetes-goodbye-spring-cloud/</guid>
      <description>I believe many developers, after familiarizing themselves with microservices, realized that they thought they had successfully built a microservices architecture empired with Spring Cloud. But after the popular of kubernetes (K8S), they were curious and exciting of creating the cloud native microservices serivces.
The Era of Spring Boot and Cloud In October 2012, Mike Youngstrom created a feature request in Spring Jira to support a containerless web application architecture in the Spring Framework.</description>
    </item>
    
    <item>
      <title>Spark SQL in Depth</title>
      <link>https://datafibers-community.github.io/blog/2021/04/28/2021-04-28-spark-sql-in-depth-copy/</link>
      <pubDate>Wed, 28 Apr 2021 20:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2021/04/28/2021-04-28-spark-sql-in-depth-copy/</guid>
      <description>In this article, we&amp;rsquo;ll look at how Spark SQL working on data queries in depth.
Checking Execution Plan Data Preparing create database if not exists test; create table if not exists test.t_name (name string); insert into test.t_name values (&#39;test1&#39;),(&#39;test2&#39;),(&#39;test3&#39;);  Test Code Preparing Below Scala code is used with testing with blocking at the standard input at the end. In this case, we can see more details from Spark WebUI.</description>
    </item>
    
    <item>
      <title>Apache Spark 3.1.1 Released :)</title>
      <link>https://datafibers-community.github.io/blog/2021/03/10/2021-03-10-spark-3.3.1-released/</link>
      <pubDate>Wed, 10 Mar 2021 20:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2021/03/10/2021-03-10-spark-3.3.1-released/</guid>
      <description>Apache Spark 3.1.1 is released on March 2, 2021. It is milestone release for Spark in 2021. This version of spark keeps making it more efficient and stable. Below are highlighted new features and changes.
 Python usability ANSI SQL compliance Query optimization enhancements Shuffle hash join improvements History Server support of structured streaming  Project Zen Project Zen was initiated in this release to improve PySpark’s usability in these three ways:</description>
    </item>
    
    <item>
      <title>Apache Superset:)</title>
      <link>https://datafibers-community.github.io/blog/2021/01/22/2021-01-22-apache-superset/</link>
      <pubDate>Fri, 22 Jan 2021 20:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2021/01/22/2021-01-22-apache-superset/</guid>
      <description>On January 21, 2021, Apache&amp;rsquo;s official announced that Apache® Superset™ has become a top-level project. Apache® Superset™ is a modern big data exploration and visualization platform that allows users to build dashboards quickly and easily using a simple code-free visualization builder and the most advanced SQL editor. The project was launched on Airbnb in 2015 and entered the Apache incubator in May 2017. Apache Superset is a big data-related BI visualization tool.</description>
    </item>
    
    <item>
      <title>Spark SQL Read/Write HBase</title>
      <link>https://datafibers-community.github.io/blog/2020/01/01/2020-01-19-spark-sql-read-write-hbase/</link>
      <pubDate>Wed, 01 Jan 2020 20:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2020/01/01/2020-01-19-spark-sql-read-write-hbase/</guid>
      <description>Apache Spark and Apache HBase are very commonly used big data frameworks. In many senarios, we need to use Spark to query and analyze the big volumn of data in HBase. Spark has wider support to read data as dataset from many kinds of data source. To read from HBase, Spark provides TableInputFormat, which as following disadvantages.
 There is only on scan triggerred in each task to read from HBase TableInputFormat does not support BulkGet Cannot leverage the optimization from Spark SQL catalyst  Considering the above points above, there is another choice by using Hortonworks/Cloudera Apache Spark—Apache HBase Connector short for (SHC).</description>
    </item>
    
    <item>
      <title>Apache Kafka Producers</title>
      <link>https://datafibers-community.github.io/blog/2019/11/02/2019-11-02-apache-kafka-producers/</link>
      <pubDate>Sat, 02 Nov 2019 19:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2019/11/02/2019-11-02-apache-kafka-producers/</guid>
      <description>Kafka producers send records to topics. The records are sometimes referred to as messages.
The producer picks which partition to send a record to per topic. The producer can send records round-robin. The producer could implement priority systems based on sending records to certain partitions based on the priority of the record. Generally speaking, producers send records to a partition based on the record’s key. The default partitioner for Java uses a hash of the record’s key to choose the partition or uses a round-robin strategy if the record has no key.</description>
    </item>
    
    <item>
      <title>Use Redish Lock for SecKill</title>
      <link>https://datafibers-community.github.io/blog/2019/05/20/2019-05-20-use-redis-lock-for-seckill/</link>
      <pubDate>Mon, 20 May 2019 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2019/05/20/2019-05-20-use-redis-lock-for-seckill/</guid>
      <description>What is Seckill? When associated with online shopping, &amp;ldquo;seckill&amp;rdquo; refers to the quick sell out of newly-advertised goods. If you look at the transaction record, you will find that each of the transactions is made in seconds. It sounds inconceivable but is the naked truth. This is called &amp;ldquo;seckill&amp;rdquo;.
A typical system for seckill has following features. * A large number of users will be shopping at the same time during the quick sell, and the web site traffic increses dramatically.</description>
    </item>
    
    <item>
      <title>Apache Kafka Consumers</title>
      <link>https://datafibers-community.github.io/blog/2018/08/04/2018-08-04-apache-kafka-consumers/</link>
      <pubDate>Sat, 04 Aug 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/08/04/2018-08-04-apache-kafka-consumers/</guid>
      <description>Kafka consumer is what we use quite often to read data from Kafka. Here, we use this article to explain some key concepts and topics regarding to consumer architecture in Kafka.
Consumer Groups We can always group consumers into a consumer group by use case or function of the group. One consumer group might be responsible for delivering records to high-speed, in-memory microservices while another consumer group is streaming those same records to Hadoop.</description>
    </item>
    
    <item>
      <title>Naive Bayes Algorithm</title>
      <link>https://datafibers-community.github.io/blog/2018/03/10/2018-03-10-ml-naive-bayes/</link>
      <pubDate>Sat, 10 Mar 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/03/10/2018-03-10-ml-naive-bayes/</guid>
      <description>Background It would be difficult and practically impossible to classify a web page, a document, an email or any other lengthy text notes manually. This is where Naïve Bayes Classifier machine learning algorithm comes to the rescue. A classifier is a function that allocates a population’s element value from one of the available categories. For instance, Spam Filtering is a popular application of Naïve Bayes algorithm. Spam filter here, is a classifier that assigns a label Spam or Not Spam to all the emails.</description>
    </item>
    
  </channel>
</rss>