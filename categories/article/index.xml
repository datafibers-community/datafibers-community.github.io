<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Article on DataFibers</title>
    <link>https://datafibers-community.github.io/categories/article/</link>
    <description>Recent content in Article on DataFibers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 May 2018 13:50:46 +0200</lastBuildDate>
    
	<atom:link href="https://datafibers-community.github.io/categories/article/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Run Hive 1 and 2 Together</title>
      <link>https://datafibers-community.github.io/blog/2018/05/30/2018-05-30-hive-1-and-2-setup/</link>
      <pubDate>Wed, 30 May 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/05/30/2018-05-30-hive-1-and-2-setup/</guid>
      <description>Overview The latest HDP 2.6.x has both Hive version 1 and 2 installed together. However, it does not allow user to run hive version to command directly, but only use beeline. The lab_dev repository here provides an demo virtual box image to have both Hive version configured properly.
Conf. Changes The trick thing to make both hive version working is do not add any setting in the .profile anymore. See below, I comments out all pervious hive settings.</description>
    </item>
    
    <item>
      <title>HBase Shell Reference</title>
      <link>https://datafibers-community.github.io/blog/2018/04/28/2018-04-28-hbase-shell_example/</link>
      <pubDate>Sat, 28 Apr 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/04/28/2018-04-28-hbase-shell_example/</guid>
      <description>We use this place to collect commonly used HBase shell command for reference. HBase shell is an HBase extensible jruby-based (JIRB) shell to execute some commands(each command represents one functionality) in HBase. HBase shell commands are mainly categorized into 6 parts as follows. Will keep adding more examples here.
1. General Information status Show cluster status. Can be &amp;lsquo;summary&amp;rsquo;, &amp;lsquo;simple&amp;rsquo;, or &amp;lsquo;detailed&amp;rsquo;. The default is &amp;lsquo;summary&amp;rsquo;.
hbase&amp;gt; status hbase&amp;gt; status &#39;simple&#39; hbase&amp;gt; status &#39;summary&#39; hbase&amp;gt; status &#39;detailed&#39;  version Output this HBase version.</description>
    </item>
    
    <item>
      <title>ML Overview</title>
      <link>https://datafibers-community.github.io/blog/2018/03/02/2018-03-02-ml-overview/</link>
      <pubDate>Fri, 02 Mar 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/03/02/2018-03-02-ml-overview/</guid>
      <description>Background Machine learning is a field of computer science that gives computer systems the ability to &amp;ldquo;learn&amp;rdquo; with data, without being explicitly programmed. Machine learning can be broken down into three broad categories: Recommender, Classification, Clustering.
 Recommender—Recommender systems suggest items based on past behavior or interest. These items can be other users in a social network, or products and services in retail websites. There are some algorithm like Pearson correlation and euclidean distance.</description>
    </item>
    
    <item>
      <title>Hive Get the Max/Min</title>
      <link>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</link>
      <pubDate>Fri, 02 Feb 2018 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2018/02/02/2018-02-02-hive-get-max-min-value-rows/</guid>
      <description>Most of time, we need to find the max or min value of particular columns as well as other columns. For example, we have following employee table.
DESC employee; +---------------+------------------------------+----------+--+ | col_name | data_type | comment | +---------------+------------------------------+----------+--+ | name | string | | | work_place | array&amp;lt;string&amp;gt; | | | gender_age | struct&amp;lt;gender:string,age:int&amp;gt;| | | skills_score | map&amp;lt;string,int&amp;gt; | | | depart_title | map&amp;lt;string,array&amp;lt;string&amp;gt;&amp;gt; | | +---------------+------------------------------+----------+--+ 5 rows selected (0.</description>
    </item>
    
    <item>
      <title>Hive RowID Generation</title>
      <link>https://datafibers-community.github.io/blog/2017/11/02/2017-11-02-hive-rowid-generation/</link>
      <pubDate>Thu, 02 Nov 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/11/02/2017-11-02-hive-rowid-generation/</guid>
      <description>Introduction It is quite often that we need a unique identifier for each single rows in the Apache Hive tables. This is quite useful when you need such columns as surrogate keys in data warehouse, as the primary key for data or use as system nature keys. There are following ways of doing that in Hive.
ROW_NUMBER() Hive have a couple of internal functions to achieve this. ROW_NUMBER function, which can generate row number for each partition of data.</description>
    </item>
    
    <item>
      <title>GIT Tips</title>
      <link>https://datafibers-community.github.io/blog/2017/11/01/2017-11-01-git-tips/</link>
      <pubDate>Wed, 01 Nov 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/11/01/2017-11-01-git-tips/</guid>
      <description>1. Git Cheat Sheet 2. Check in Git Modified But Untracked Content Recently, I migrate this site to Hexo. I download the theme from github to the Hexo project folder. I also keep the source code in the github in case I lost the source code. However, when I run the git add . and git status. It shows error messages saying the theme folder is not tracked content. Most time, I did not check the git status - bad habit.</description>
    </item>
    
    <item>
      <title>Apache Kafka Overview</title>
      <link>https://datafibers-community.github.io/blog/2017/10/05/2017-10-05-apache-kafka-overview/</link>
      <pubDate>Thu, 05 Oct 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/10/05/2017-10-05-apache-kafka-overview/</guid>
      <description>The big data processing started by focusing on the batch processing. Distributed data storage and querying tools like MapReduce, Hive, and Pig were all designed to process data in batches rather than continuously. Recently enterprises have discovered the power of analyzing and processing data and events as they happen instead of batches. Most traditional messaging systems, such as RabbitMq, neither scale up to handle big data in realtime nor use friendly with big data ecosystem.</description>
    </item>
    
    <item>
      <title>Scala Apply Method</title>
      <link>https://datafibers-community.github.io/blog/2017/09/15/2017-09-15-scala-apply-method/</link>
      <pubDate>Fri, 15 Sep 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/09/15/2017-09-15-scala-apply-method/</guid>
      <description>The apply methods in scala has a nice syntactic sugar. It allows us to define semantics like java array access for an arbitrary class.
For example, we create a class of RiceCooker and its method cook to cook rice. Whenever we need to cook rice, we could call this method.
class RiceCooker { def cook(cup_of_rice: Rice) = { cup_of_rice.isDone = true cup_of_rice } } val my_rice_cooker: RiceCooker = new RiceCooker() my_rice_cooker.</description>
    </item>
    
    <item>
      <title>One Platform Initatives for Spark</title>
      <link>https://datafibers-community.github.io/blog/2017/06/24/2017-06-24-cloudera-launches-one-platform-initatives-to-advance-spark---copy/</link>
      <pubDate>Sat, 24 Jun 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/06/24/2017-06-24-cloudera-launches-one-platform-initatives-to-advance-spark---copy/</guid>
      <description>In the early of this September, the Chief Strategy Offer of Cloudera Mike Olson has announced that the next important initiatives for Couldera - One Platform to advance their investment on Apache Spark.
The Spark is originally invented by few guys who started up the Databrick. Later, Spark catches most attention from big data communities and companies by its high-performance in-memory computing framework, which can run on top of Hadoop Yarn.</description>
    </item>
    
    <item>
      <title>Constructor - Scala vs. Java</title>
      <link>https://datafibers-community.github.io/blog/2017/05/01/2017-05-01-scala-and-java-constructors/</link>
      <pubDate>Mon, 01 May 2017 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2017/05/01/2017-05-01-scala-and-java-constructors/</guid>
      <description>1. Constructor With Parameters Java Code
public class Foo() { public Bar bar; public Foo(Bar bar) { this.bar = bar; } }  Scala Code
class Foo(val bar:Bar)  2. Constructor With Private Attribute Java Code
public class Foo() { private final Bar bar; public Foo(Bar bar) { this.bar = bar; } }  Scala Code
class Foo(private val bar:Bar)  3. Call Super Constructor Java Code
public class Foo() extends SuperFoo { public Foo(Bar bar) { super(bar); } }  Scala Code</description>
    </item>
    
    <item>
      <title>When to Disable Speculative Execution</title>
      <link>https://datafibers-community.github.io/blog/2016/11/22/2016-11-22-when-to-disable-speculative-execution/</link>
      <pubDate>Tue, 22 Nov 2016 13:50:46 +0200</pubDate>
      
      <guid>https://datafibers-community.github.io/blog/2016/11/22/2016-11-22-when-to-disable-speculative-execution/</guid>
      <description>Backgrounds This is the link from WikiMedia about what’s Speculative Execution. In Hadoop, the following parameters string are for this settings. And, they are true by default.
mapred.map.tasks.speculative.execution mapred.reduce.tasks.speculative.execution  When to Disable Most time, it helps. However, I am here to collect some scenario when we do not need it.
 Of course, when ever your cluster really in shortage of resource or for the purpose of experiment, we can disable them by setting them to false since “SE” really a big resource consumer It is generally advisable to turn off ”SE” for mapred jobs that use HBase as a source.</description>
    </item>
    
  </channel>
</rss>